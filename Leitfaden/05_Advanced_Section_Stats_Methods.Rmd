---
editor_options: 
  markdown: 
    wrap: sentence
---

\newpage

# Seasonality and Geographic Differences -- Application of Statistical Methods

## Merging Bees and Weather

Hopefully the first part was easy for the more advanced of you. Regardless, let's continue with some more challenging exercises!

We want to investigate how diseases and different climates affect our beloved bee populations.
To that end, you now need to merge the two data sets "Bees" and "Weather", and we will no longer be using the data that we have previously merged.
As in the previous exercie, both data sets have different frequencies, can you spot the difference? To merge you will need to aggregate the daily weather data set to quarterly data. Be carefull when aggregating the data though, it might not be clever to replace period-specific max/min values with a mean!

If you get lost, you might want to compare your approach to your solution of [4.4.1](#Merging).

Please note that there is no weather data for **Florida** and **New Mexico**, you should exclude these states.

## Correlations

### Naive Approach

After merging the data sets into, let's start with some analysis. To cover our basis, we would like to start off by plotting a simple correlations plot for the variables of interest. You can pick which variables you would like to plot!
This could be lost colonies, different parasites, and weather indicators for example.

```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.align="center", out.width = "100%"}
knitr::include_graphics("plot/Correlations using Pearson Method.jpg")
```

-   We have a question for you: Why is a naive correlation matrix across states likely a bad idea? Think for example about omitted variables and how information is lost when you aggregate fundamentally different groups.

::: {.tips .r data-latex="r"}
Use the `cor()` function from the `base` R to create a correlation matrix. Select all numerical variables in your data set with the help of `sapply()` or`dplyr`’s `select()` and create a correlation matrix.

Alternatively, use the `ggcorrplot()` from the same-name [{ggcorrplot}](https://cran.r-project.org/web/packages/ggcorrplot/index.html) package.

:::

::: {.tipsp .python data-latex="p"}
A handy library for plotting correlation matrices is the [seaborn library](https://seaborn.pydata.org/):

    import seaborn as sns
    ...

You can use its `pairplot` method and pass on the data frame with the selected columns to visualize distributions and correlations.

Alternatively/Additionally, you may want to plot a heatmap with `sns.heatmap(...)` which makes it even easier to see correlations.
:::

### Statewise Correlations - Seasonality & Geography

Having checked a very simplistic correlation, we need to do some thinking. Our cross-section of states is characterized by heterogeneous geography and climate.
Naturally, this also implies seasonality in our quarterly data set.
However, due to the large szie of the United States, we must not forget that for some southern states, in our data Hawaii's, seasons are less pronounced than they are in North Dakota (if you do not believe us, check the max temperature differential ...).

To visualize this, you want to compute statewise correlations for either "Lost Colonies", "log Lost Colonies" or "percent Lost Colonies" and sort them by latitude. 
As always, do not forget to label the graphs in a meaningful way, and please explain how the results of your plot can be interpreted to your audience!

Below are some examples:

```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.align="center", out.width = "100%"}
knitr::include_graphics("plot/State Correlations sorted by Latitude - Lost Colonies and Quarter Dummies ex Hawaii.jpg")
```

```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.align="center", out.width = "100%"}
knitr::include_graphics("plot/State Correlations sorted by Latitude - Lost Colonies Percent and Quarter Dummies ex Hawaii.jpg")
```
Note, that there might also be interaction effects, e.g. the season/climate influences the parasite population or the like.

```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.align="center", out.width = "100%"}
knitr::include_graphics("plot/Seasonal (temperature-related) Parasite Infestation.jpg")
```

::: {.tips .r data-latex="r"}
We recommend you using the tidyverse approach. From previous exercises you should be well versed in using `dplyr` commands. 
A combination of `group_by()`, `filter()`, `select()`,  and `arrange()` should do the job nicely!
If you unsure, check back with the dplyr cheat sheet.
:::

::: {.tipsp .python data-latex="p"}
You should be fairly familiar with `groupby` statements by now. To sort by an underlying property of the index values, it might be of help to calculate correlations in one data frame and get a list of ordered states from another using `.reset_index()`, `.drop_duplicates()`, and `.sort_values()`. Subsequently, you can easily use `.reindex()`.
:::

### Demeaned Variables
Create at least two variables that you want to use in the following regressions which can mitigate some of the issues touched above:

* One variable that subtracts a quarter aggregate (median, mean ...) over ALL states (4 values in total) from each observation in that respective quarter

**Example:** You calculate the average percentage of Varroa Mites infestation in each quarter for all states together. Subsequently, you subtract the average quarter 1 value from all states' quarter 1 values for Varroa Mites, the average quarter 2 value from all states' quarter 2 values, and so on ...


* One measure that subtracts a quarter aggregate by state AND quarter (values equal to the number of states * 4) from all observations with the respective state and quarter

**Example:** You create a measure of "abnormal" temperature. First, average the temperature grouping by state and quarter. Subsequently subtract this state- and quarter-dependent measure from each matching quarter and state.
There are 6 years, so you will average over 6 first quarters for a given state and subsequently subtract that average from each of these 6 observations. 

Additionally, add quarter dummies to your data frame. E.g. 4 columns where in each row only a single column is 1.

::: {.tips .r data-latex="r"}
Add tip
:::

::: {.tipsp .python data-latex="p"}
You best work with a `Multiindex` and employ the `.dt.quarter` property of `Datetime` columns.
After grouping and aggregating, you can merge the values back as an additional column and simply subtract as well as reassign the result.
:::

## Pooled OLS for State Panel Data
As we investigated correlation patterns above, we saw that panel data is not straightforward to analyze. Especially since we are only equipped with quarterly data of mere 6 years - that's not a lot.
But you have just added a few more "normalized" variables, so we can get started with our regressions in the hope to make some interesting findings.

With panel data, the basic extension of OLS (Ordinary Least Squares) regression is Pooled OLS.
A basic Pooled OLS regression equation could look like this (a simple extension of classic OLS with state effects $\alpha_i$ and error term $\epsilon_{i,t}$:

$$
percent\_lost\_colonies_{i, t} = \mu + \beta_{1,i} \cdot varroa\_mites_{i, t} + \beta_2 \cdot quarter_1 + \dots + \alpha_i + \epsilon_{i,t}
$$

### Seasonality Specification
Compute pooled OLS regression estimates that reflect only seasonal aspects.
Subsequently, you will extend this model below. At the end of this section is a [#table] that may serve as an indication of what the end result could look like.

::: {.tips .r data-latex="r"}
Though you have learned to use `lm` in your DataCamp courses, you will be using the package `lfe` here, as we want to extend the model from a simple OLS model to a pooled OLS model for panel data. But do not worry, the syntax is kept close to the one of `lm`!

In order to extend your OLS model, you will have to cluster your data for season (what variable do you have that can be compared to seasons?). To do so, have a look at the [documentation](https://rdrr.io/cran/lfe/man/felm.html). To help you out and eliminate unnecessary specifications for you, the `felm` command should look a little bit like this:

   felm(formula | 0 | 0 | clustervar= , data= )
   
However, feel free to specify the model differently after having read the documentation, and let us know what you have done!
To create a table with which you can compare your different regression models, use the `stargazer` package and specify `type="html"`.
:::

::: {.tipsp .python data-latex="p"}
A classic package for inference is [statsmodels](https://www.statsmodels.org/stable/generated/statsmodels.regression.linear_model.OLS.html#statsmodels.regression.linear_model.OLS). Standard OLS is pretty straightforward but not needed here. 
[linearmodels](https://pypi.org/project/linearmodels/) is an extension library that supports the analysis of panel data.
:::

### Adding Geographic Aspects
Extend your specification from above with at least one variable that reflects each states "unique" geography. 
Whenever you add more variables be careful to avoid (multi-) collinearity (= your regressors are highly correlated among themselves).

### Adding indicators for weather and parasite infestation
Last but not least add variables (maybe some of the ones created above) that can explain even more, considering for example weather extrema and parasite infestation.

When you compare all the $R^2$ of the different specifications with special attention to *within* and *between* $R^2$ what do you observe?

```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.align="center", out.width = "100%"}
knitr::include_graphics("plot/Regression_Table_POLS_only.PNG")
```

## Alternative Model 
You are completely free here. You want to check out a machine-learning approach and compare? This is your chance!
All you have to do is estimate an alternative model (so not pooled OLS again) whose properties you can compare in a later stage.

## Model Evaluation
Model evaluation is always important, even more so when using different frameworks that you would like to compare.
At the end of the day, we are interested in the model that reflects our empirical observations best.

### Residuals Distribution (in-sample)
When computing regression estimates the residuals as unexplained fraction of your model are the most important aspect to look at. Especially since each model assumes a certain distribution (most of the time we assume a normal distribution). Since we just assume a distribution, we must check whether these assumptions hold.
Typical related questions are whether they are auto-correlated and whether the standard errors computer are robust (e.g. test for the presence of heteroscedasticity etc.). 

Plot the residuals of one of your pooled OLS specifications and the ones of your alternative model. Do they differ fundamentally?

::: {.tips .r data-latex="r"}
As you are working with the `lfe` package, specifically the `felm` command, all your model outputs are in a `felm`-object. Therefore, unlike using `lm`, you do not need to estimate your model after you have created it as this is done automatically. To get a brief overview, click on the model in the environment. If you are not sure what you are looking for the plot, check back with the `felm`-documentation. 
To help you out, you can extract the parts you need from the `felm`-object with the following command:

x <- model[["x"]]

After extracting everything you need, create a data frame and plot using `ggplot`.
:::

::: {.tipsp .python data-latex="p"}
Your fitted models will have something like a `.predict()` method which also yields the in-sample fitted values. Moreover, the fitted model will automatically compute the residuals, for example the `.resids` attribute that you can work with. 
Of course you could also use the regression estimates to compute both values yourself!
:::

```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.align="center", out.width = "100%"}
knitr::include_graphics("plot/Pooled OLS Aggregate Residuals.jpg")
```

### Model Selection
As a wrap-up, by considering the different specifications as well as your alternative model, write a few sentences about which specification/model you believe to be best-suited for the underlying data.

**Congratulations!** You’ve made it to the end of your TechAcademy Data Science project. After visualizing the data in the first part, you’ve also set up a statistical analysis in this section. If you stuck around until this part and managed to code the better part of the exercises, you have definitely earned your certificate! We hope you had fun learning Data Science with this combination of data sets and enjoyed it – at least the parts where you did not get stuck forever because of some unexplainable coding error. Do not forget to send your project results to our project submission email address before the deadline (**03.07.2022, 23:59**).
Thank you for being a part of TechAcademy!
